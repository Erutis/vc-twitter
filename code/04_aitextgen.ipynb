{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pARJTmo2fVf"
      },
      "source": [
        "# GPT-2-Simple aitextgen\n",
        "\n",
        "It is recommended to run this notebook in Google Colab. \n",
        "\n",
        "\n",
        "\n",
        "This is a shortened version of [this Colab notebook](https://colab.research.google.com/drive/15qBZx5y9rdaQSyWpsreMDnTiZ5IlN0zD?usp=sharing).\n",
        "It takes an already fine-tuned model and generates text with it. \n",
        "\n",
        "\n",
        "This model uses OpenAI's GPT-2 and EleutherAI's GPT Neo/GPT-3 architecture. For more about `aitextgen`, you can visit [this GitHub repository](https://github.com/minimaxir/aitextgen) or [read the documentation](https://docs.aitextgen.io/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI6v5UIq2aVy",
        "outputId": "c058944b-7316-43e6-d55a-d193db756534"
      },
      "outputs": [],
      "source": [
        "#!pip install -q aitextgen\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s — %(levelname)s — %(name)s — %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO\n",
        "    )\n",
        "\n",
        "from aitextgen import aitextgen\n",
        "from aitextgen.colab import mount_gdrive, copy_file_from_gdrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzjzpaTN2yZD"
      },
      "source": [
        "# Load a trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7piGlXi3hCv",
        "outputId": "5bfd0303-3b9c-4c75-daa1-f5b1bce1d579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "CdeHKcEA2lxJ",
        "outputId": "d39d7b63-b1e2-4e06-baae-1e7d9f3948b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "06/09/2022 12:43:58 — INFO — aitextgen — Loading model from provided weights and config in //Users/kei/Downloads/checkpoint/run1.1.\n",
            "06/09/2022 12:44:02 — INFO — aitextgen — GPT2 loaded with 124M parameters.\n",
            "06/09/2022 12:44:02 — INFO — aitextgen — Using the default GPT-2 Tokenizer.\n"
          ]
        }
      ],
      "source": [
        "ai = aitextgen(model_folder=\"/Users/kei/Downloads/checkpoint/run1.1\", to_gpu=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kpSvec021GI"
      },
      "source": [
        "# Generate text from trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0YOQXKqcAZb"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = ai.generate_one()`\n",
        "\n",
        "You can also pass in a `prompt` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `n`. You can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 50 for `batch_size` to avoid going OOM).\n",
        "\n",
        "Other optional-but-helpful parameters for `ai.generate()` and friends:\n",
        "\n",
        "*  **`min length`**: The minimum length of the generated text: if the text is shorter than this value after cleanup, aitextgen will generate another one.\n",
        "*  **`max_length`**: Number of tokens to generate (default 256, you can generate up to 1024 tokens with GPT-2 and 2048 with GPT Neo)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5igV4H32luj",
        "outputId": "fe4b6e1e-ed2c-4626-a827-ac54bba53536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'s as if you are in or adjacent to VC and aren't actively working to wrest power from VC's ossified, patriarchal power structure, you aren't doing the same work as the small number of brave insiders, you aren't doing anything besides profiting from the exploitation of the overlooked.\n"
          ]
        }
      ],
      "source": [
        "ai.generate(max_length=80, min_length=20, temperature=.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrSdTQB92lsN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of 04_text_gen.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "249521544297c6ec507a72ba15cf7a321abfdee091769d75ff45105dfcf5fa97"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('aitext_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
