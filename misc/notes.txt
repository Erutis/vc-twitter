What is needed for this:

- twitter downloaded, cleaned, vectorized
- transfer learning for BERT or something
- figure out how to fine-tune using twitter text 
- could also try to train it ourselves
- run code that generates text 
- streamlit/tkinter app to generate text 



Libraries:
- pandas/numpy/os/re/warnings
- configparser
- tweepy
- matplotlib
- scikit-learn
- gpt-2-simple
- streamlit
- time 


Log

5/20/22 
- Got access to the Twitter API and began trying to download tweets. 


5/23/22
- Trying to clean and anonymize the tweets I have downloaded. 
- Trying to find other users with a good tweets. 


5/29/22
- Finally got username removal down and URL removal done. That was satisfying to get the regex to work right. 


6/1/22
- Watching OpenAI videos to see how to generate text. Still haven't figured it out.
- Tried running HuggingFace's GPT-2, but something went wrong with the transformers library install. 


6/4/22
- Trying out EleuthAI's GPT-3 copy. Big download - 22 GBs for the model. Doing it on Colab. 
- Ok, Colab runs out of memory trying to load the model...... I'll try this again at home, maybe on a different computer? 
- Meanwhile, attempting to make Streamlit app. Lol, TLaaS: Thought Leadership as a Service
- Dustin suggested creating a use case where a user can choose who they want to emulate and whether they want a blog or tweet

- Welp, EleuthAI won't work. It takes 24 GB of ram and I do not have such capabilities.
- Got GPT-2 to work, but now need to figure out how to fine-tune it using my Twitter data. Looking up the Harry Potter AI chapter for inspiration.
- Max Woolf has a colab notebook that is set up for easy fine-tuning. I'll make a copy and see if it works.
- Ok, fine-tuning using Max Woolf's gpt_2_simple. Early results are hilarious. 
- It ran for at least 40 minutes and gave me the samples from sample_gpt2.txt 
- I now have a fine-tuned model! But it has a lot of quotes " . Should try to get rid of them. 
- I've removed " quotes and 'RT :' from the text. Now rerunning the fine-tuning


6/6/22
- Going to concentrate on bridging the model and streamlit app. 
- Snags: trying to run gpt-2-simple on my machine errors out regardless of using .py or a notebook. Internet says this is a M1 problem. 
- It's fine, I'll just make do with the Colab notebook. It worked fine on there.


- - - - - 
- Ok, after troubleshooting with Devin & Caroline for an hour, here's what was accomplished:
- - - - Error: illegal hardware instruction: solved by running these steps:
- - - - - 1. https://developer.apple.com/metal/tensorflow-plugin/ Run this in terminal: 

"""
chmod +x ~/Downloads/Miniforge3-MacOSX-arm64.sh
sh ~/Downloads/Miniforge3-MacOSX-arm64.sh
source ~/miniforge3/bin/activate
"""

This will create a new 'base' kind of with miniforge. THEN follow the rest of the instructions

2. Install tensorflow dependencies: conda install -c apple tensorflow-deps
3. Install base tensorflow: python -m pip install tensorflow-macos
4. Install tensorflow-metal plugin: python -m pip install tensorflow-metal

- - - - - 

- After that, had to reinstall gpt-2-simple and had to uninstall and reinstall numpy for some reason. All in the miniforge env, which is apparently separate from base.
- After all that, the .py file is finally working, it's just not doing what I meant it to do. 
- I removed the model training and only left the pulling of the run2 model output. It worked!! It gave me back this:

'who is john galt? john galt is a legend in and around vc. 
i don't know who this guy is but i do know that vcs are screwed over by past'

- It's taking about 3 minutes to run and takes up a ton of RAM while it runs. 
- I confirmed it isn't the model loading that takes the most time, but the generation part. :( 

- Besides optimization, next step is figuring out how to tie Streamlit button to a function.
- Ok, so using the on_click param, it works. It just doesn't output the text generation to the app... only the terminal.

- I tried adding params, but it just breaks it. :( Also Ben couldn't help with Streamlit or with optimization. He suggested I reach out to Niraj Saran from a few cohorts ago. 
- Tomorrow's task will be trying to see how I can output the text to the app and how I can limit the memory used.


6/7/22
- Worked with Eric trying to figure out optimization, but the real answer may just be to use a Google Colab nb to speed up the generation.
- I've got a working colab notebook that draws the fine-tuned model from my Google drive
- I also redownloaded everyone's Tweets, this time setting it to the last 3,000 for each user and trained a run3 model on that. The results are less funny. I might keep using run2. 
- Nice! @cache() sped up the text generation by 50%. However, it also means the button can only be pressed once. 
- Jeff helped with session_state, which finally got the text gen to print in the app instead of the terminal
- The app is pretty janky - I don't have a way to prevent the error for ['text'] before pressing the button. 

- Taking a break from the app to focus on cleaning up the other notebooks.
- README and notebooks cleaned as of 5pm on 6/7/22
- Tomorrow and Thursday I'll focus on two things: 1) smoothing out the streamlit app OR running it in colab, and 2) the presentation


6/8/22
- Can't do much without internet on the plane, but I did separate out short tweets and long tweets based on if they had fewer than 20 characters. 
- When I get to an internet connection, I'll start the Colab notebook on a run4 that only has longer tweets to eat. 
- Also for the error in the streamlit app, I should try some try/except things to avoid it. 
- I made a requirements text file while I wait for run4 to finish
- I added a try/except to hide the error from streamlit/app.py's session_state['text'] line


For future note:
checkpoints: 
- run1 - the original fine-tuning that had the RT and quotes; used only 300 tweets per user
- run2 - second run using the 300 posts/user. took out RT and quotes and it looks cleaner.
- run3 - third run, this time using 3,000 posts/user. results are more matter-of-fact and less funny.
- run4 - tried excluding tweets that had fewer than 20 characters to help the output be long each time. 


For tomorrow:
- update readme with run info
- add link to the runs in Drive
- implement the new max wolf model
- see if huggingface upload works
- start thinking through prez and collecting our fave outputs
- 'want to sound like a real VC? Get TLaaS.'
- watch spark 2
- possibly take the cleaning functions and put them in a separate .py file 
- make a model from scratch


6/9/22 
- Priorities: made vc-twitter-update to try huggingface & the new model.
- notes will be in there in case anything breaks

-- notes from copy of repo while trying aitextgen --

- run2.1 is using 300 tweets/user on the new aitextgen using the file.txt
- run3.1 will use the 3000_tweets.csv one
- run4.1 will use the long_text.txt
- run 1.1 will use the original 300 tweets in a .csv to avoid the spacing weirdness.

- it works great in the colab notebook. Barely 2-3 seconds per generation. The csv-trained one is working pretty well
- Streamlit online won't work - it doesn't want to take the google drive link to the model and I don't have another way to host it.

- I might just use the aitextgen library instead for the whole app. We'll see if it works.
- Ok, that works like a DREAM. Less than 10 seconds for generation, less than 10 seconds to laod the model. Amazing.
- Removed the st.cache() because it doesn't even need it. Speedy little guy. That made the button work on each click. 

- LOCAL APP DONE!!!! 

- Now to figure out how to get it to run online....
- going to try the google URl again with aitextgen this time. Please work omg.
- Nope. nothing has a good way to load the .bin. It won't recognize that the filepath is correct.
- I've spent 2 whole hours on this 
- Caroline with the save!!! It was "." That was the filepath it wanted. It doesn't need the .json necessarily.
- It's freaking working - live!!! 

For Friday:
- Clean up the Readme, upload the different versions
- make presentation
- watch spark 2


6/10/22

Additional things we could do:
- Include [name] and [url] instead of just removing when anonymizing
- Remember which run you uploaded...Or just upload another. 

- added in [username] and [url] to the 3000 tweets. Training now. 
- run5.1 added. This is using 3000/user and has [url] and [username]
- added a prompt ability to the HF app; seems to have fixed it. Also temp=.9 now.


- for future reminder - the environment used for the local streamlit app is aitext_env


6/11/22

- finished presentation
- finished updated readme